# @package _global_
defaults:
  - example.yaml
  - override /hydra/launcher: custom_submitit_slurm

name: debug_submitit_launcher
seed: ${oc.env:SLURM_PROCID,123}
log_level: DEBUG

trainer:
  accelerator: gpu
  devices: 1
  max_epochs: 1

hydra:
  verbose: True
  mode: MULTIRUN
  launcher:
    nodes: 1
    cpus_per_gpu: 4
    gpus: rtx8000:1
    # TODO: Packing more than one job on a single GPU seems to be working, but both tasks get the
    # same job parameters.
    mem_gb: 16
    array_parallelism: 2  # max num of jobs to run in parallel

    srun_args: ["--overcommit"]  # to share CPU cores between tasks

    max_vram_usage_gb: 5
    gpu_model: rtx8000
    parallel_runs_per_job: 2

    # Other things to pass to `sbatch`:
    additional_parameters:
      time: 1-00:00:00  # maximum wall time allocated for the job (D-HH:MM:SS)

# hydra:
#   launcher:
#     additional_parameters:
#       nodes: 1
#     cpus_per_gpu: 4
#     gpus: rtx8000:1
#     mem: 16G
#     # ntasks_per_node: 1
#     srun_args: ["--overcommit"]  # to share CPU cores between tasks

#     # NOTE: `mem_per_task` is a good way to think about this according to @obilaniu, but it
#     # unfortunately isn't a flag of `sbatch`. You can get the same result by setting `mem_per_cpu`:
#     # `mem_per_cpu = "mem_per_task" / cpus_per_task`
#     # In this example, we want "mem_per_task" of 16G:
#     mem_per_cpu: "${int_divide:16,${.cpus_per_task}}G"
